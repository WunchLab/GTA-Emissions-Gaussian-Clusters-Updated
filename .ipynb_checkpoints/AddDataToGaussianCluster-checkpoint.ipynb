{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter information about new data and run the whole notebook to create a new gaussian cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5f37eb2a1db8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import folium\n",
    "import sklearn\n",
    "import sklearn.mixture\n",
    "import random\n",
    "import SURF_peakfinder_program_Emily_Ju as peak\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import helperFunctions as hf\n",
    "import math\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose type of file you want to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment bike or truck depending on the type of data\n",
    "transport='bike'\n",
    "#transport='truck'\n",
    "if transport =='bike':\n",
    "    filter_len=500\n",
    "    threshold=0.05\n",
    "    mypath='dataBike'\n",
    "elif transport =='truck':\n",
    "    filter_len=300\n",
    "    threshold=0.05\n",
    "    mypath='dataTruck'\n",
    "\n",
    "#modify bike files to account for the 30 sec delay of the instrument    \n",
    "adjustDelay=True # False if already did it\n",
    "if adjustDelay and transport == 'bike':\n",
    "    delayedPath='dataBikeOldDelayed'\n",
    "    filesToBeAdjusted= [f  for f in listdir(delayedPath) if isfile(join(delayedPath, f)) and f[-3:]==\"csv\" ] # every day\n",
    "    #filesToBeAdjusted=['sync_data_2019-07-31.csv']\n",
    "    hf.adjustDelay(filesToBeAdjusted,delayedPath=delayedPath , adjustedPath=mypath)\n",
    "\n",
    "#choose files\n",
    "files = [f  for f in listdir(mypath) if isfile(join(mypath, f)) and f[-3:]==\"csv\" ] # every day\n",
    "#files=['sync_data_2019-07-31.csv'] # 1 (or a few) day only\n",
    "\n",
    "#from files get list of dates\n",
    "onlyDates=[x[10:20] for x in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Peak based on height and area under the curve\n",
    "Finds position and level of peaks and stores them in \"areaPeaks/area\"+dt+\".peaks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in onlyDates:\n",
    "    if transport=='bike':\n",
    "        file=mypath+\"/sync_data_\"+d+\".csv\"\n",
    "        dt=d\n",
    "    elif transport=='truck':\n",
    "        file=mypath+\"/sync_data_\"+d+\"_Truck.csv\"\n",
    "        #To identify truck data we change the '2' for a 't' in the year (2019 -> t019)\n",
    "        dt = list(d)\n",
    "        dt[0]='t'\n",
    "        dt=\"\".join(dt)\n",
    "    #find peaks and put in areaPeaks directory\n",
    "    peak.data_analy.areaPeaks(file, filter_len, threshold, dt,limA=[1,8,60], limH=[0.04,0.2,1],transport=transport, show=True)\n",
    "    #cleaned peak files\n",
    "    peak.data_analy.clean_file(\"areaPeaks/area\"+dt+\".peaks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load level 2 and 3 peaks and correct for level and wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct for level and wind\n",
    "#put peaks from dataframe to x array format\n",
    "areaPath='areaPeaks'\n",
    "#Plot bike/truck data ( add '2'/'t' to start list)\n",
    "cleanedFiles = [areaPath+\"/\"+f for f in listdir(areaPath) if isfile(join(areaPath, f)) and\n",
    "                f[-11:]=='cleaned.csv'  and f[4]=='2' and f!='area2019-07-10.peaks_cleaned.csv' and f!='area2019-07-04.peaks_cleaned.csv']\n",
    "col=['blue','red','green','orange']# color for level\n",
    "xAc=[]# list of corrected coordinates of level 2-3 peaks\n",
    "plt.figure(figsize=(5,5))\n",
    "for f in cleanedFiles:\n",
    "    peaks=pd.read_csv(f)\n",
    "    for i in range(peaks.shape[0]):\n",
    "        long, lat = peaks[\"Longitude\"][i],peaks['Latitude'][i]\n",
    "        #load wind data\n",
    "        wd=math.radians(peaks['wd_corr'][i])\n",
    "        ws=peaks['ws_corr'][i]\n",
    "        mult=5e-05# force of displacement\n",
    "        #move peaks along wind direction with factor mult*wind speed\n",
    "        corrLong= long- (mult*ws*math.cos(wd))\n",
    "        corrLat= lat- (mult*ws*math.sin(wd))\n",
    "        if peaks[\"Level\"][i]==2:\n",
    "            plt.plot(corrLong,corrLat,'.',linestyle='none',color=col[peaks[\"Level\"][i]], label=\"level \"+str(peaks[\"Level\"][i]))\n",
    "            xAc.append([corrLong,corrLat])\n",
    "        #if level 3 add more point because  peak is more important\n",
    "        if peaks[\"Level\"][i]==3:\n",
    "            for count in range(3):\n",
    "                plt.plot(corrLong,corrLat,'.',linestyle='none',color=col[peaks[\"Level\"][i]], label=\"level \"+str(peaks[\"Level\"][i]))\n",
    "                xAc.append([corrLong,corrLat])\n",
    "        \n",
    "#legend. write only once each label            \n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "plt.xlim((-79.8,-79.2))\n",
    "plt.ylim((43.55,43.9))\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "alone,xAc,_,_=hf.separate(xAc, limit=0.01)# remove alone points from gaussian mixture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Gaussian Mixture to find cluster\n",
    "This has only been tested on bike data.\n",
    "If you don't want the wind correction, make mult=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#gaussian mixture\n",
    "n=23# number of cluster\n",
    "#learn gaussians\n",
    "gmAc =sklearn.mixture.GaussianMixture(n_components=n, covariance_type='full',random_state=3715688689).fit(xAc)\n",
    "#predict label (cluster) for each point\n",
    "label=gmAc.predict(xAc)\n",
    "#plot map. color by cluster\n",
    "m=hf.clusterMap(xAc,label,n,showMeans=True, showFacilities=True,means=gmAc.means_,covariances=gmAc.covariances_,save=True, name='GaussianClusters',alone=alone)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize parameter of gaussian mixture\n",
    "Only run this once in a while to adjust the parameters if you need to have the best possible cluster analysis. The replace mult, random_state and n in the cells above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "areaPath='areaPeaks'\n",
    "cleanedFiles = [areaPath+\"/\"+f for f in listdir(areaPath) if isfile(join(areaPath, f)) and f[-11:]=='cleaned.csv'  and f[4]=='2' and f!='area2019-07-10.peaks_cleaned.csv'and f!='area2019-07-04.peaks_cleaned.csv']\n",
    "\n",
    "mult, sil, random_state, n=hf.bestParam(cleanedFiles,nRan=range(20,28), rRan=range(25),verbose=True)\n",
    "print(mult, sil, random_state, n)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
